{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import seaborn as sns\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.svm import LinearSVC\n",
    "import tqdm\n",
    "\n",
    "import pickle \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_data(data_path:str,save_path:str,file_name:str)-> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregate data in every segment for every sensor\n",
    "    After this aggregation we get one value for every sensor in segment\n",
    "\n",
    "    Parameters:\n",
    "        data_path\n",
    "        save_path\n",
    "        file_name\n",
    "    Returns:\n",
    "        pd.DataFrame \n",
    "    \"\"\"\n",
    "    \n",
    "    df_dict = {}\n",
    "    for filename in glob.glob(data_path):\n",
    "        file = pd.read_csv(filename).fillna(0)\n",
    "        ind = filename.split('/')[1].split('.')[0]\n",
    "        features = file.sum().add_suffix('_sum')\n",
    "        features = features.append(file.mean().add_suffix('_mean'))\n",
    "        features = features.append(file.std().add_suffix('_std'))\n",
    "        features = features.append(file.max().add_suffix('_max'))\n",
    "        features = features.append(file.min().add_suffix('_min'))\n",
    "        features = features.append(file.skew().add_suffix('_skew'))\n",
    "        features = features.append(file.kurtosis().add_suffix('_kurtosis'))\n",
    "        df_dict[ind] = features\n",
    "    df = pd.DataFrame(df_dict).transpose()\n",
    "    df.to_csv(save_path+file_name)\n",
    "\n",
    "def quantile_data(data_path,save_path,file_name):\n",
    "     \"\"\"\n",
    "    Aggregate data in every segment for every sensor\n",
    "    After this aggregation we get one quantile value for every sensor in segment\n",
    "\n",
    "    Parameters:\n",
    "        data_path\n",
    "        save_path\n",
    "        file_name\n",
    "    Returns:\n",
    "        pd.DataFrame \n",
    "    \"\"\"\n",
    "    df_dict = {}\n",
    "    for filename in tqdm.tqdm(list(glob.glob(data_path))):\n",
    "        file = pd.read_csv(filename).fillna(0)\n",
    "        ind = filename.split('/')[1].split('.')[0]\n",
    "        features = file.quantile(.8).add_suffix('_0.8')\n",
    "        features = features.append(file.quantile(.9).add_suffix('_0.9'))\n",
    "        features = features.append(file.quantile(.99).add_suffix('_0.99'))   \n",
    "        df_dict[ind] = features\n",
    "    df = pd.DataFrame(df_dict).transpose()\n",
    "    df.to_csv(save_path+file_name)\n",
    "    \n",
    "\n",
    "def split_train_test_with_sklearn(df:pd.DataFrame)->pd.DataFrame:\n",
    "     \"\"\"\n",
    "    Prepare train and test df\n",
    "\n",
    "    Parameters:\n",
    "        df\n",
    "    Returns:\n",
    "        train\n",
    "        test\n",
    "    \"\"\"\n",
    "    train, test  = train_test_split(df,test_size=0.33,random_state=42)\n",
    "    return train, test\n",
    "\n",
    "def prepare_for_modeling(df):\n",
    "     \"\"\"\n",
    "    Prepare data for modeling. Split df for two df: features and target\n",
    "\n",
    "    Parameters:\n",
    "        df-data frame\n",
    "    Returns:\n",
    "        X-features\n",
    "        y-target\n",
    "    \"\"\"\n",
    "    y = df['time_to_eruption']\n",
    "    #y = pd.factorize(y)[0]\n",
    "    X = df.drop(['time_to_eruption'],axis=1)\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/jesperdramsch/introduction-to-volcanology-seismograms-and-lgbm\n",
    "\n",
    "http://eqseis.geosc.psu.edu/cammon/HTML/Classes/IntroQuakes/Notes/seismometers.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate data and calculate percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aggregate_data('train/*.csv','train/','aggredated_train.csv')\n",
    "#aggregate_data('test/*.csv','test/','aggredated_test.csv')\n",
    "\n",
    "#quantile_data('train/*.csv','train/','quantiled_train.csv')\n",
    "#quantile_data('test/*.csv','test/','quantiled_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_to_eruption</th>\n",
       "      <th>sensor_1_sum</th>\n",
       "      <th>sensor_2_sum</th>\n",
       "      <th>sensor_3_sum</th>\n",
       "      <th>sensor_4_sum</th>\n",
       "      <th>sensor_5_sum</th>\n",
       "      <th>sensor_6_sum</th>\n",
       "      <th>sensor_7_sum</th>\n",
       "      <th>sensor_8_sum</th>\n",
       "      <th>sensor_9_sum</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor_1_0.99</th>\n",
       "      <th>sensor_2_0.99</th>\n",
       "      <th>sensor_3_0.99</th>\n",
       "      <th>sensor_4_0.99</th>\n",
       "      <th>sensor_5_0.99</th>\n",
       "      <th>sensor_6_0.99</th>\n",
       "      <th>sensor_7_0.99</th>\n",
       "      <th>sensor_8_0.99</th>\n",
       "      <th>sensor_9_0.99</th>\n",
       "      <th>sensor_10_0.99</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>segment_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1136037770</th>\n",
       "      <td>12262005</td>\n",
       "      <td>-96621.0</td>\n",
       "      <td>276834.0</td>\n",
       "      <td>213587.0</td>\n",
       "      <td>121201.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-583141.0</td>\n",
       "      <td>423609.0</td>\n",
       "      <td>76103.0</td>\n",
       "      <td>87904.0</td>\n",
       "      <td>...</td>\n",
       "      <td>704.0</td>\n",
       "      <td>1755.0</td>\n",
       "      <td>733.0</td>\n",
       "      <td>904.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1518.0</td>\n",
       "      <td>1247.0</td>\n",
       "      <td>1420.0</td>\n",
       "      <td>899.0</td>\n",
       "      <td>1375.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1969647810</th>\n",
       "      <td>32739612</td>\n",
       "      <td>85569.0</td>\n",
       "      <td>149069.0</td>\n",
       "      <td>-167659.0</td>\n",
       "      <td>-102036.0</td>\n",
       "      <td>43927.0</td>\n",
       "      <td>-538513.0</td>\n",
       "      <td>352219.0</td>\n",
       "      <td>-65211.0</td>\n",
       "      <td>188411.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1108.0</td>\n",
       "      <td>1740.0</td>\n",
       "      <td>902.0</td>\n",
       "      <td>904.0</td>\n",
       "      <td>737.0</td>\n",
       "      <td>845.0</td>\n",
       "      <td>1207.0</td>\n",
       "      <td>846.0</td>\n",
       "      <td>1022.0</td>\n",
       "      <td>2494.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1895879680</th>\n",
       "      <td>14965999</td>\n",
       "      <td>150278.0</td>\n",
       "      <td>326988.0</td>\n",
       "      <td>-95314.0</td>\n",
       "      <td>-69051.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-799715.0</td>\n",
       "      <td>-117460.0</td>\n",
       "      <td>-246701.0</td>\n",
       "      <td>-110002.0</td>\n",
       "      <td>...</td>\n",
       "      <td>613.0</td>\n",
       "      <td>2240.0</td>\n",
       "      <td>527.0</td>\n",
       "      <td>711.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1037.0</td>\n",
       "      <td>828.0</td>\n",
       "      <td>774.0</td>\n",
       "      <td>646.0</td>\n",
       "      <td>1231.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2068207140</th>\n",
       "      <td>26469720</td>\n",
       "      <td>129950.0</td>\n",
       "      <td>-22403.0</td>\n",
       "      <td>-161942.0</td>\n",
       "      <td>-79013.0</td>\n",
       "      <td>18528.0</td>\n",
       "      <td>-50214.0</td>\n",
       "      <td>-50589.0</td>\n",
       "      <td>-10519.0</td>\n",
       "      <td>-201797.0</td>\n",
       "      <td>...</td>\n",
       "      <td>556.0</td>\n",
       "      <td>1116.0</td>\n",
       "      <td>467.0</td>\n",
       "      <td>579.0</td>\n",
       "      <td>424.0</td>\n",
       "      <td>732.0</td>\n",
       "      <td>716.0</td>\n",
       "      <td>766.0</td>\n",
       "      <td>563.0</td>\n",
       "      <td>1339.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192955606</th>\n",
       "      <td>31072429</td>\n",
       "      <td>4429.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>119935.0</td>\n",
       "      <td>14705.0</td>\n",
       "      <td>-26483.0</td>\n",
       "      <td>132341.0</td>\n",
       "      <td>-287066.0</td>\n",
       "      <td>-103821.0</td>\n",
       "      <td>97376.0</td>\n",
       "      <td>...</td>\n",
       "      <td>636.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>553.0</td>\n",
       "      <td>635.0</td>\n",
       "      <td>469.0</td>\n",
       "      <td>1007.0</td>\n",
       "      <td>691.0</td>\n",
       "      <td>940.0</td>\n",
       "      <td>682.0</td>\n",
       "      <td>1334.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            time_to_eruption  sensor_1_sum  sensor_2_sum  sensor_3_sum  \\\n",
       "segment_id                                                               \n",
       "1136037770          12262005      -96621.0      276834.0      213587.0   \n",
       "1969647810          32739612       85569.0      149069.0     -167659.0   \n",
       "1895879680          14965999      150278.0      326988.0      -95314.0   \n",
       "2068207140          26469720      129950.0      -22403.0     -161942.0   \n",
       "192955606           31072429        4429.0           0.0      119935.0   \n",
       "\n",
       "            sensor_4_sum  sensor_5_sum  sensor_6_sum  sensor_7_sum  \\\n",
       "segment_id                                                           \n",
       "1136037770      121201.0           0.0     -583141.0      423609.0   \n",
       "1969647810     -102036.0       43927.0     -538513.0      352219.0   \n",
       "1895879680      -69051.0           0.0     -799715.0     -117460.0   \n",
       "2068207140      -79013.0       18528.0      -50214.0      -50589.0   \n",
       "192955606        14705.0      -26483.0      132341.0     -287066.0   \n",
       "\n",
       "            sensor_8_sum  sensor_9_sum  ...  sensor_1_0.99  sensor_2_0.99  \\\n",
       "segment_id                              ...                                 \n",
       "1136037770       76103.0       87904.0  ...          704.0         1755.0   \n",
       "1969647810      -65211.0      188411.0  ...         1108.0         1740.0   \n",
       "1895879680     -246701.0     -110002.0  ...          613.0         2240.0   \n",
       "2068207140      -10519.0     -201797.0  ...          556.0         1116.0   \n",
       "192955606      -103821.0       97376.0  ...          636.0            0.0   \n",
       "\n",
       "            sensor_3_0.99  sensor_4_0.99  sensor_5_0.99  sensor_6_0.99  \\\n",
       "segment_id                                                               \n",
       "1136037770          733.0          904.0            0.0         1518.0   \n",
       "1969647810          902.0          904.0          737.0          845.0   \n",
       "1895879680          527.0          711.0            0.0         1037.0   \n",
       "2068207140          467.0          579.0          424.0          732.0   \n",
       "192955606           553.0          635.0          469.0         1007.0   \n",
       "\n",
       "            sensor_7_0.99  sensor_8_0.99  sensor_9_0.99  sensor_10_0.99  \n",
       "segment_id                                                               \n",
       "1136037770         1247.0         1420.0          899.0          1375.0  \n",
       "1969647810         1207.0          846.0         1022.0          2494.0  \n",
       "1895879680          828.0          774.0          646.0          1231.0  \n",
       "2068207140          716.0          766.0          563.0          1339.0  \n",
       "192955606           691.0          940.0          682.0          1334.0  \n",
       "\n",
       "[5 rows x 101 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read dfs with aggregated data and merge to one df\n",
    "time = pd.read_csv('train.csv')\n",
    "df1 = pd.read_csv('aggregated_train.csv')\n",
    "df2 = pd.read_csv('quantiled_train.csv')\n",
    "df1 = df1.rename(columns = {'Unnamed: 0':'segment_id'}) \n",
    "df2 = df2.rename(columns = {'Unnamed: 0':'segment_id'})# 'segment_id' object-> float64\n",
    "df = pd.merge(time,df1, on=['segment_id'])\n",
    "df = pd.merge(df,df2,on='segment_id')\n",
    "df = df.set_index('segment_id')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4431, 101)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['time_to_eruption', 'sensor_1_sum', 'sensor_2_sum', 'sensor_3_sum',\n",
       "       'sensor_4_sum', 'sensor_5_sum', 'sensor_6_sum', 'sensor_7_sum',\n",
       "       'sensor_8_sum', 'sensor_9_sum',\n",
       "       ...\n",
       "       'sensor_1_0.99', 'sensor_2_0.99', 'sensor_3_0.99', 'sensor_4_0.99',\n",
       "       'sensor_5_0.99', 'sensor_6_0.99', 'sensor_7_0.99', 'sensor_8_0.99',\n",
       "       'sensor_9_0.99', 'sensor_10_0.99'],\n",
       "      dtype='object', length=101)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to sum all f.exp.sum/mean for every columns\n",
    "#column_name_for_sum = set([col.split('_')[-1] for col in df.columns if col != 'time_to_eruption'])\n",
    "#for col_name in column_name_for_sum:\n",
    "        #df[col_name] = df[[col for col in df.columns if col.endswith('_{}'.format(col_name))]].mean(axis=1)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model with feature selection or without"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kasia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.0411406617084208e+17, tolerance: 53098442976163.04\n",
      "  positive)\n"
     ]
    }
   ],
   "source": [
    "# split train/test\n",
    "train,test = split_train_test_with_sklearn(df)\n",
    "print('Train shape is {}, test size is {}'.format(train.shape,test.shape))\n",
    "\n",
    "X_train,y_train = prepare_for_modeling(train)\n",
    "X_test,y_test = prepare_for_modeling(test)\n",
    "\n",
    "# list of models\n",
    "lin_reg = LinearRegression()\n",
    "lasso = Lasso(alpha = 0.5)\n",
    "dec_tree = DecisionTreeRegressor()\n",
    "ran_for = RandomForestRegressor()\n",
    "xgb_meta = XGBRegressor(colsample_bytree=0.4,\n",
    "                             gamma=0,\n",
    "                            learning_rate=0.07,\n",
    "                            max_depth=3,\n",
    "                            min_child_weight=1.5,\n",
    "                            n_estimators=1000,\n",
    "                            reg_alpha=0.75,\n",
    "                            reg_lambda=0.45,\n",
    "                            subsample=0.6,\n",
    "                            seed=2)\n",
    "models = [lin_reg,lasso,dec_tree,ran_for,xgb_meta ]\n",
    "\n",
    "results = []\n",
    "model_res = {}\n",
    "for model in models:\n",
    "    clf = Pipeline([('feature_selection', SelectFromModel(RandomForestRegressor(),threshold='median')),\n",
    "                      ('classification', model)\n",
    "                    ])\n",
    "    clf.fit(X_train,y_train)\n",
    "    #pred for train\n",
    "    y_pred_train = clf.predict(X_train)\n",
    "    MSA_train = mean_absolute_error(y_train,y_pred_train)\n",
    "\n",
    "    # pred for test\n",
    "    y_pred = clf.predict(X_test)\n",
    "    MSA_test = mean_absolute_error(y_test,y_pred)\n",
    "    model_res[str(model).split('(')[0]] = [int(MSA_train),int(MSA_test)]\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAE train with feature importance</th>\n",
       "      <th>MAE test with feature importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LinearRegression</th>\n",
       "      <td>9893229</td>\n",
       "      <td>10584854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso</th>\n",
       "      <td>9953408</td>\n",
       "      <td>10541933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTreeRegressor</th>\n",
       "      <td>0</td>\n",
       "      <td>4967707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestRegressor</th>\n",
       "      <td>1635996</td>\n",
       "      <td>4389219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBRegressor</th>\n",
       "      <td>2140012</td>\n",
       "      <td>4863029</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       MAE train with feature importance  \\\n",
       "LinearRegression                                 9893229   \n",
       "Lasso                                            9953408   \n",
       "DecisionTreeRegressor                                  0   \n",
       "RandomForestRegressor                            1635996   \n",
       "XGBRegressor                                     2140012   \n",
       "\n",
       "                       MAE test with feature importance  \n",
       "LinearRegression                               10584854  \n",
       "Lasso                                          10541933  \n",
       "DecisionTreeRegressor                           4967707  \n",
       "RandomForestRegressor                           4389219  \n",
       "XGBRegressor                                    4863029  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_dict(model_res, orient='index',\n",
    "                       columns=['MAE train with feature importance','MAE test with feature importance'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression and Lasso have big Mean Absolute Error on train and test set. In this case we deal with underfitting. Our error ist similar and very high. In this case we can try to use more complex model or we can try to find better features. We also can to try to regulize the model. \n",
    "\n",
    "In the Decsion Tree Regressor case the MAE on train set is 0. In this case our overfitted the data. On train data we got better results in comparison to Linear Regression and Lasso.\n",
    "\n",
    "Train and test reults for Random Forest and XGBRegressor shows that the models are overfitted\n",
    "In this case we need to regulize the model, use less complex model or reduce numer of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check which features are important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sensor_1_std', 'sensor_2_std', 'sensor_3_std', 'sensor_4_std',\n",
       "       'sensor_5_std', 'sensor_6_std', 'sensor_8_std', 'sensor_9_std',\n",
       "       'sensor_1_max', 'sensor_2_max', 'sensor_3_max', 'sensor_5_max',\n",
       "       'sensor_6_max', 'sensor_1_min', 'sensor_2_min', 'sensor_3_min',\n",
       "       'sensor_5_min', 'sensor_6_min', 'sensor_9_min', 'sensor_1_kurtosis',\n",
       "       'sensor_2_kurtosis', 'sensor_5_kurtosis', 'sensor_6_kurtosis',\n",
       "       'sensor_9_kurtosis', 'sensor_10_kurtosis', 'sensor_1_0.8',\n",
       "       'sensor_2_0.8', 'sensor_3_0.8', 'sensor_4_0.8', 'sensor_5_0.8',\n",
       "       'sensor_6_0.8', 'sensor_7_0.8', 'sensor_8_0.8', 'sensor_10_0.8',\n",
       "       'sensor_1_0.9', 'sensor_2_0.9', 'sensor_3_0.9', 'sensor_5_0.9',\n",
       "       'sensor_6_0.9', 'sensor_7_0.9', 'sensor_8_0.9', 'sensor_9_0.9',\n",
       "       'sensor_10_0.9', 'sensor_1_0.99', 'sensor_2_0.99', 'sensor_5_0.99',\n",
       "       'sensor_6_0.99', 'sensor_7_0.99', 'sensor_8_0.99', 'sensor_9_0.99'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#features importance\n",
    "clf.steps[0][1].get_support()\n",
    "X_train.columns[clf.steps[0][1].get_support()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we show 50% of all features which are most important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alternative for evaluation to train/test\n",
    "X_df_cross,y_df_cross= prepare_for_modeling(df)\n",
    "scores = cross_val_score(ran_for,X_df_cross,y_df_cross,scoring='neg_mean_absolute_error',cv=5)\n",
    "print('Mean MAE for cross val score is {}'.format((-scores).mean()))\n",
    "print('Std for MAE for cross val score is {}'.format((-scores).std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative to train/test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid search for parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "df_x = pd.merge(time,df2, on=['segment_id'])\n",
    "df_x = df_x.set_index('segment_id')\n",
    "\n",
    "train,test = split_train_test_with_sklearn(df_x)\n",
    "\n",
    "X_train,y_train = prepare_for_modeling(train)\n",
    "X_test,y_test = prepare_for_modeling(test)\n",
    "\n",
    "param_grid = [{\n",
    "    'n_estimators':[150],\n",
    "    'max_depth':[15],\n",
    "    'min_child_weight': [1],\n",
    "    'subsample': [1],\n",
    "    'colsample_bytree': [1],\n",
    "    'learning_rate':[0.10],\n",
    "    # Other parameters\n",
    "    'objective':['reg:squarederror']}]\n",
    "\n",
    "\n",
    "xgb_cl = XGBRegressor()\n",
    "grid_search = GridSearchCV(xgb_cl,param_grid,cv=5, scoring = 'neg_mean_absolute_error')\n",
    "grid_search.fit(X_train,y_train)\n",
    "\n",
    "final_model = grid_search.best_estimator_\n",
    "final_model = grid_search.best_estimator_\n",
    "y_pred = final_model.predict(X_test)\n",
    "MSA = mean_absolute_error(y_test,y_pred)\n",
    "print('MSA: {}'.format(MSA))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preapre data for submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read test file from kaggle\n",
    "df1 = pd.read_csv('aggredated_test.csv')\n",
    "df2 = pd.read_csv('test/quantiled_test.csv')\n",
    "df1 = df1.rename(columns = {'Unnamed: 0':'segment_id'}) \n",
    "df2 = df2.rename(columns = {'Unnamed: 0':'segment_id'})# 'segment_id' object-> float64\n",
    "df_test = pd.merge(df1,df2,on='segment_id')\n",
    "df_test = df_test.set_index('segment_id')\n",
    "df_test\n",
    "\n",
    "#whole dataset for train and test from kaggle\n",
    "X_df,y_df = prepare_for_modeling(df)\n",
    "\n",
    "clf = Pipeline([\n",
    "  #('feature_selection', SelectFromModel(RandomForestRegressor(),threshold=0.012)),\n",
    "  ('classification', XGBRegressor())\n",
    "])\n",
    "\n",
    "clf.fit(X_df,y_df)\n",
    "y_pred = clf.predict(df_test)\n",
    "\n",
    "# results submission \n",
    "data = {'segment_id':df_test.index,\n",
    "        'time_to_eruption':y_pred}\n",
    "forsub = pd.DataFrame(data)\n",
    "forsub = forsub.astype(int)\n",
    "sub = pd.read_csv('sample_submission.csv')\n",
    "sub = sub.drop('time_to_eruption',axis=1)\n",
    "submission_file = pd.merge(sub,forsub, on='segment_id')\n",
    "#submission_file.to_csv('submission_file_XGB_3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best submission was XGBoost with file only with quantile\n",
    "Leaderboard position 196"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trash -> First version for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train/test\n",
    "train,test = split_train_test_with_sklearn(df)\n",
    "print('Train shape is {}, test size is {}'.format(train.shape,test.shape))\n",
    "\n",
    "X_train,y_train = prepare_for_modeling(train)\n",
    "X_test,y_test = prepare_for_modeling(test)\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train,y_train)\n",
    "y_pred = lin_reg.predict(X_test)\n",
    "MSA = mean_absolute_error(y_test,y_pred)\n",
    "print('Linear Regression MSA: {}'.format(MSA))\n",
    "\n",
    "lasso = Lasso(alpha = 0.5)\n",
    "lasso.fit(X_train,y_train)\n",
    "y_pred = lasso.predict(X_test)\n",
    "MSA = mean_absolute_error(y_test,y_pred)\n",
    "print('Lasso MSA: {}'.format(MSA))\n",
    "\n",
    "dec_tree = DecisionTreeRegressor()\n",
    "dec_tree.fit(X_train,y_train)\n",
    "y_pred = dec_tree.predict(X_test)\n",
    "MSA = mean_absolute_error(y_test,y_pred)\n",
    "print('Decision Tree MSA: {}'.format(MSA))\n",
    "\n",
    "ran_for = RandomForestRegressor()\n",
    "ran_for.fit(X_train,y_train)\n",
    "y_pred = ran_for.predict(X_test)\n",
    "MSA = mean_absolute_error(y_test,y_pred)\n",
    "print('Random Forest Regressor MSA: {}'.format(MSA))\n",
    "\n",
    "\n",
    "xgb_meta = XGBRegressor(colsample_bytree=0.4,\n",
    "                             gamma=0,\n",
    "                            learning_rate=0.07,\n",
    "                            max_depth=3,\n",
    "                            min_child_weight=1.5,\n",
    "                            n_estimators=1000,\n",
    "                            reg_alpha=0.75,\n",
    "                            reg_lambda=0.45,\n",
    "                            subsample=0.6,\n",
    "                            seed=2)\n",
    "xgb_meta.fit(X_train,y_train)\n",
    "y_pred = xgb_meta.predict(X_test)\n",
    "MSA = mean_absolute_error(y_test,y_pred)\n",
    "print('XGBRegressor MSA: {}'.format(MSA))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
